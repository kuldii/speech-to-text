{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf29254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip-compile --strip-extras requirements.in\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42a90ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 20:52:44.781168: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eada2683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "from torchaudio.datasets import LIBRISPEECH\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Cek device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load dataset LibriSpeech (dev-clean)\n",
    "train_dataset = LIBRISPEECH(\"./datasets\", url=\"dev-clean\", download=False) # Jika blm punya dataset, bisa set True\n",
    "test_dataset = LIBRISPEECH(\"./datasets\", url=\"test-clean\", download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4cd4f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/124 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "  1%|          | 1/124 [00:01<04:03,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 0, Loss: 5.141490936279297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 11/124 [00:12<01:56,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 10, Loss: 1.4457511901855469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 21/124 [00:24<02:03,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 20, Loss: 1.6058954000473022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 31/124 [00:36<01:54,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 30, Loss: 1.5224261283874512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 41/124 [00:47<01:35,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 40, Loss: 1.7254657745361328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 51/124 [01:00<01:27,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 50, Loss: 1.5493769645690918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 61/124 [01:13<01:22,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 60, Loss: 1.7625221014022827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 71/124 [01:26<01:01,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 70, Loss: 2.7678775787353516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 81/124 [01:37<00:46,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 80, Loss: 1.147099494934082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 91/124 [01:48<00:35,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 90, Loss: 0.9281482100486755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 101/124 [01:59<00:26,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 100, Loss: 0.797355592250824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 111/124 [02:11<00:14,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 110, Loss: 1.242854356765747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 121/124 [02:21<00:03,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 120, Loss: 1.0097301006317139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124/124 [02:24<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(1):  # num_train_epochs\n",
    "    for i, (waveform, sample_rate, transcript, _, _, _) in enumerate(tqdm(train_dataset)):\n",
    "        # Preprocessing input\n",
    "        inputs = processor(waveform.squeeze().numpy(), sampling_rate=sample_rate, return_tensors=\"pt\")\n",
    "        input_features = inputs.input_features.to(device)\n",
    "        labels = processor.tokenizer(transcript, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_features=input_features, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Step {i}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8316e25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted transcript:  AND WHAT ALORMENTS OR WHAT VANTUGE IS UPON THE FOREHEAD OF THE OTHER SHOWED THAT VAL SHOULD'S TURN THY FOOTSTEPPES UNTO THEM\n",
      "Reference transcript: AND WHAT ALLUREMENTS OR WHAT VANTAGES UPON THE FOREHEAD OF THE OTHERS SHOWED THAT THOU SHOULDST TURN THY FOOTSTEPS UNTO THEM\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the waveform for the model\n",
    "inputs = processor(waveform.squeeze().numpy(), sampling_rate=sample_rate, return_tensors=\"pt\")\n",
    "input_features = inputs.input_features.to(device)\n",
    "\n",
    "# Generate prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_ids = model.generate(input_features)\n",
    "\n",
    "# Decode the prediction\n",
    "predicted_transcript = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "print(\"Predicted transcript:\", predicted_transcript)\n",
    "print(\"Reference transcript:\", transcript)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
